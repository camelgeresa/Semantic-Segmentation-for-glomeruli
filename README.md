# Identifying function tissue units (FTUs) in human kidney tissue images
The dataset used for this project is from a Kaggle competition :https://www.kaggle.com/competitions/hubmap-kidney-segmentation, where the aim was to identify FTUs in the kidney tissue images. 
Specifically, "Your challenge is to detect functional tissue units (FTUs) across different tissue preparation pipelines. An FTU is defined as a “three-dimensional block of cells centered around a capillary, such that each cell in this block is within diffusion distance from any other cell in the same block” (de Bono, 2013). The goal of this competition is the implementation of a successful and robust glomeruli FTU detector" (from the website)
# Background:
![image](https://user-images.githubusercontent.com/99748864/178031713-4d218448-edae-4851-a2a3-d67615e51ed2.png)

The kidney is made up of many nephrons, which all work together to filter the 150 quarts of blood passing through the kidneys each day (https://www.niddk.nih.gov/health-information/kidney-disease/kidneys-how-they-work). At the opening of each nephron, there is a network of small blood vessels through which the blood is filtered, and where the filtrate is passed through the rest of the nephron to produce urine. By detecting these FTUs automatically, this could help researchers improve their understanding of the relationships between cells and cellular activity occuring in the kidneys. 

The following is an example of a kidney tissue image:
![image](https://user-images.githubusercontent.com/99748864/178030920-561b89d9-22ff-42c2-b657-7fe55b09ca91.png)

#Process and challenges
Being a beginner in computer vision, this challenge has thrown a lot of challenges my way. Firstly, the images in this dataset were very big and I would have to load them in a memory efficient way. Additionally I would have to resize them and crop them to enable them to pass through a segmentation model, such as Unet which wouldn't be able to work on such large images. Finally, the masks were stored in rle format, which I have previously never worked with. Such a problem required more than the classification problems I have previously worked on. 
Initially, I tried to load the images, one by one as this was more memory efficient than loading them all in at once (using a for loop). I then created a custom dataset class (with tf keras as a parent class), where the images would be resized to 768x768. I would then take 512x512 crops. I did this instead of automatically resizing to 512, as this was a good opportunity for data albumentations, where this would help the model generalise to different types of images and hence prevent overfitting. Finally the batches would be taken. For the rle decoding, there were many functions on kaggle that were available to use. However, I simply did not have enough memory for this data, no matter the method I tried, so I eventually decided on using another dataset, with the same data that was already resized and that would take up less memory. 
From there, I wrote custom datasets, dataloaders and transforms using PyTorch. For the model, I implemented transfer learning ,i.e used pre-made architectures that have been shown to be superior for semantic segmentation tasks. I opted for the FPN model, instead of the UNet as I already was familiar with the Unet and I wanted to try something new. 
